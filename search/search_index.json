{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"REMIND-PyPSA Coupling Tools","text":"<p>A package to support the coupling of REMIND and PyPSA, written by the Potsdam Institute for Climate Impact Studies' Energy Transition Lab</p>"},{"location":"#abstracted-logic","title":"Abstracted logic","text":"<p>The package provides an abstracted framework for ETL operations based steps. The steps can be read from YAML into the transformation class</p>"},{"location":"#remind-data-support","title":"Remind data support","text":"<p>Can be imported either as csv or gdx. Both formats can be processed (gdx not yet integrated in the examples)</p>"},{"location":"#built-in-transformations-and-disaggregation","title":"Built in transformations and disaggregation","text":"<p>The package comes with tools to transform and disaggregate remind data.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Examples and tutorials to help you understand the package</p>"},{"location":"objects/","title":"Data and objects","text":"<p>This page introduces the objects and data expected by the coupling layer</p>"},{"location":"objects/#data","title":"Data","text":""},{"location":"objects/#mapping-table","title":"Mapping Table","text":"<p>The REMIND-PyPSA mapping allows to build the the techno-economic data from the pypsa and remind techno-economic data </p> Variable Description Allowed values example PyPSA_tech pypsa tech names as expected by network creation <code>OCGT</code> parameter the techno-economic field <code>[CO2 itensity, efficiency, FOM, fuel, investment, lifetime, VOM]</code> - mapper func to determine the techno-econ parameter <code>[\"set_value\", \"use_remind\", use_pypsa\", \"weigh_remind_by_gen\", \"use_remind_with_learning_from\"]</code> - reference the value to pass to the mapper <code>set_value: 1.2, use_pysa:(ignored), use_remind: biochp (remind_name), weigh_remind_by_gen: \"[biochp, bioigcc]\"</code> unit the reference unit. Note pypsa load cost sometimes uses this! str, missing USD/Mwh, % (for FOM) comment additional comments that will be added to pypsa_cost csv entry str \"Dummy value to avoid issues\" <p>Example:</p> PyPSA_tech parameter mapper reference unit comment battery inverter CO2 intensity set_value 0 tCO2/MWh_th biomass investment weigh_remind_by_gen \"[biochp, bioigcc, bioigccc]\" <p>The pypsa &lt;-&gt; REMIND tech name map is derived from this table (using the investment parameter by default).</p>"},{"location":"objects/#remind-data","title":"REMIND Data","text":"<p>The following data are needed from REMIND:</p> <ul> <li>techno-economic parameters: <code>pm_data</code></li> <li>weights for technology baskets: e.g <code>p32_weightGen</code>,</li> <li>CO2 prices: <code>p_priceCO2</code></li> <li>pre-investment capacities (several options)</li> <li>AC load: <code>p32_load</code></li> <li>capex: <code>p32_capCost</code> (several options),</li> <li>eta: <code>pm_dataeta</code>,: <code>pm_eta_conv</code>,</li> <li>fuel_costs: <code>p32_PEPriceAvg</code>,</li> <li>discount_r: <code>p32_discountRate</code>,</li> <li>co2_intensity: <code>pm_emifac</code>,</li> <li>run name: <code>c_expname</code></li> <li>version: <code>c_modelversion</code></li> </ul> <p>These can either be exported in the gdx format or a series of csvs. Implicit in these are the years and regions.</p>"},{"location":"objects/#pypsa-data","title":"PyPSA data","text":"<ul> <li>the cost data: in case the pypsa-cost data ends before the remind horizon, the missing values will be fixed to the last available time point</li> <li>the existing infrastructure data (powerplantmatching or equiv)</li> </ul>"},{"location":"objects/#objects","title":"Objects","text":""},{"location":"objects/#transformations-abstracted-etl","title":"transformations (abstracted etl)","text":"<p>Transformation steps are represented by the Transformation dataclass. These can be specified in YAML</p> <pre><code>steps:\n    name: example\n    method: run_example\n    frames: \n        data1: pm_data1 # remind name\n        eta: p32_etaconversion\n    params: #extra params\n    filters:\n        eta: \"region==@region\" # pandas queries for the table\n    kwargs: null # extra args\n</code></pre> <p>The method name points to the ETL register. You can declare your own transformations with the <code>@register_etl</code> decorator from <code>rpycpl.etl</code> example</p>"},{"location":"objects/#data_1","title":"data","text":"<p>Each of these data are represented as pandas: <code>pd.Dataframe</code> or: <code>pd.Series</code>. </p> <p>Whilst there is validation - in particular of the mapping - the package expects column names as per powerplantmatching, pypsa and REMIND (GAMS sets in this case).</p> <p>The various routines also merge pypsa and remind data. Some processing functions will only worked with the merged data. The merged data should have <code>suffixes=(\"_remind\", \"_pypsa\")</code>.</p>"},{"location":"transformations/","title":"Transformation Methods","text":""},{"location":"transformations/#techno-economic-data","title":"Techno-economic data","text":"<p>This builds the coupled techno-economic data for the PyPSA model from REMIND parameters. Alternatively, values can be directly set or taken from PyPSA. For each target parameter (key pair), a mapping method can be specified (in order of preference):</p> <ul> <li>use_remind: use the remind tech specified in the \"reference column\"</li> <li>use_remind_weighed_by: aggregate different remind technologies together using a weight (eg generation share)</li> <li>set_value: directly set the value to that specified in the \"reference\" column</li> <li>use_pypsa: use the pypsa name. The \"reference\" column has no effect</li> <li>use_remind_with_learning_from: this is intended for technologies that are not implemented in REMIND. The base year cost will be set from pypsa and the cost will decrease in time as per the REMIND technology specified in the \"reference\" column. This method is only valid for \"investment\" costs</li> </ul>"},{"location":"transformations/#loads-data","title":"Loads data","text":"<ul> <li>conversion to MWh</li> </ul>"},{"location":"transformations/#spatial-disaggregation","title":"Spatial Disaggregation","text":"<ul> <li>pre-defined reference data</li> </ul>"},{"location":"tutorials/","title":"Examples","text":""},{"location":"tutorials/#abstracted-logic","title":"abstracted logic","text":"<p>You can find simple examples of using the abstracted methods in the examples folder. These have trivial transformations</p> <p>These include:</p> <ul> <li><code>basic_script.py</code>: a basic abstracted ETL logic based on a <code>yaml</code> description of the steps, with fake local data. </li> <li><code>read_and_transform_remind.py</code>: as above but using a data loader. The example loads fake csv data  generated as part of the example.</li> <li><code>custom_transformations.py</code> an example of how to register custom transformations </li> </ul> <p>The examples are unfortunately untested.</p>"},{"location":"tutorials/#direct-scripts","title":"Direct scripts","text":"<p>if you do not want to use abstracted logic, a one-off script example </p> <ul> <li><code>one-offscript.py</code>: a script using the csv reader</li> </ul>"},{"location":"tutorials/#exporting-remind-data","title":"Exporting REMIND data","text":"<p>There are three possibilities to export data from GAMS:</p> <ol> <li>make your own GDX and export only the needed symbols (params, sets, ..). You may want to make sub-sets and new symbols to control the export. </li> <li>Use the <code>embeddedCode</code> and <code>gamsconnect</code> functionalities to export to csv. This has the advantage that the data is immediately readable to external users.</li> <li>Use the <code>fulldata.gdx</code> - not recommended as too implicit.</li> </ol> <p>Example for option 2:</p> <pre><code>EmbeddedCode Python:\n\"\"\"\nProgramatic loop over gams connect CSV export for all export PARAMS\n\"\"\"\nimport yaml, os\nfrom gams.connect import ConnectDatabase\n\n\nPARAMS = [\"part1\", \"set1\"]\n\n\ndir = \"./pypsa_export\"\nif not os.path.isdir(dir):\n    os.mkdir(dir)\n\n# single gams connect yaml nstruction\nexport_instruct = '''\n    - GAMSReader:\n        symbols:\n          - name: {par}\n    - CSVWriter:\n        file: {dir}/{par}.csv\n        name: {par}\n        valueSubstitutions: {'EPS': 0}\n    '''\n\n# Loop\ncdb = ConnectDatabase(gams._system_directory, ecdb=gams)\nfor par in PARAMS:\n    par_instr = yaml.safe_load(export_instruct.replace(\"{par}\",par).replace(\"{dir}\",dir))\n    try:\n        # the gams connect export\n        cdb.execute(par_instr)\n    except Exception as e:\n        gams.printLog(f\"Error par {par} skipped: {e}\")\n\nendEmbeddedCode\n</code></pre>"},{"location":"tutorials/#integration-with-snakemake","title":"Integration with snakemake","text":"<p>The idea is to use a snakemakre rule's <code>snakemake.params</code>, <code>snakemake.inputs</code> and <code>snakemake.outputs</code> + a section of your config (loaded from yaml) to control execution.</p> <ol> <li>the config has the ETL transformation steps. The config can be added to the snakefile or passed via the CLI using <code>--configfile=&lt;myfile&gt;</code> </li> <li>the data locations are from <code>snakemake.inputs</code> (idem for outputs)</li> </ol> <p>We recommend splitting the ETL and disagg step into two rules. If you want a more explicit representation in the workflow, you can use the same one or two script with many rules and different arguments.</p>"},{"location":"docs/reference/capacities_etl/","title":"Capacities etl","text":""},{"location":"docs/reference/capacities_etl/#capacities_etl--extract-transform-load-etl-operations-for-remind-pre-invetment-generation-capacities","title":"Extract, Transform, Load (ETL) operations for REMIND (pre-invetment) generation capacities","text":"<p>The aim is to translate the REMIND pre-investment capacities into pypsa brownfield capacities. PyPSA workflows already come with their own bronwfield data (e.g. from powerplantmatching) assigned  to nodes/clusters. This capacity needs to be adjusted to the REMIND capacities.</p>"},{"location":"docs/reference/capacities_etl/#capacities_etl--harmonisation-of-remind-and-pypsa-capacities","title":"Harmonisation of REMIND and PypSA Capacities","text":"<p>In case the REMIND capacities are smaller than the pypsa brownfield capacities,      the pypsa capacities are scaled down by tech.</p> <p>In case the REMIND capacities are larger, the pypsa brownfield capacities are kept and an       additional paid-off component is added to the pypsa model as a max (paid-off ie free)      capacity constraint. The constraint is REMIND REGION wide so that pypsa       determines the optimal location of the REMIND-built capacity. </p>"},{"location":"docs/reference/capacities_etl/#capacities_etl--workflow-integration","title":"Workflow integration","text":"<p>The constraints and data are exported as files made available to the pypsa workflow.</p>"},{"location":"docs/reference/capacities_etl/#capacities_etl--reference-guide","title":"Reference Guide","text":""},{"location":"docs/reference/capacities_etl/#capacities_etl.calc_paidoff_capacity","title":"<code>calc_paidoff_capacity(merged)</code>","text":"<p>Calculate the aditional paid off capacity available to pypsa from REMIND investment decisions. This capacity is not geographically allocated within the remind region network.</p> <p>Parameters:</p> Name Type Description Default <code>merged</code> <code>DataFrame</code> <p>DataFrame with the merged remind and pypsa capacities by tech group.</p> required <p>Returns:     pd.DataFrame: DataFrame with the available paid off capacity by tech group.</p> Source code in <code>src/rpycpl/capacities_etl.py</code> <pre><code>def calc_paidoff_capacity(merged: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the aditional paid off capacity available to pypsa from REMIND investment decisions.\n    This capacity is not geographically allocated within the remind region network.\n\n    Args:\n        merged (pd.DataFrame): DataFrame with the merged remind and pypsa capacities by tech group.\n    Returns:\n        pd.DataFrame: DataFrame with the available paid off capacity by tech group.\n    \"\"\"\n    merged.fillna(0, inplace=True)\n    merged[\"paid_off\"] = merged.capacity_remind - merged.capacity_pypsa\n    merged.paid_off = merged.paid_off.fillna(0).clip(lower=0)\n    return merged.groupby(\"tech_group\").paid_off.sum()\n</code></pre>"},{"location":"docs/reference/capacities_etl/#capacities_etl.scale_down_pypsa_caps","title":"<code>scale_down_pypsa_caps(merged_caps, pypsa_caps, tech_groupings)</code>","text":"<p>Scale down the pypsa capacities to match the remind capacities by tech group. Does not scale up the pypsa capacities.</p> <p>Scaling is done by groups of techs, which allows n:1 mapping of remind to pypsa techs.</p> <p>Parameters:</p> Name Type Description Default <code>merged_caps</code> <code>DataFrame</code> <p>DataFrame with the merged remind and pypsa capacities by tech group.</p> required <code>pypsa_caps</code> <code>DataFrame</code> <p>DataFrame with the pypsa capacities.</p> required <code>tech_groupings</code> <code>DataFrame</code> <p>DataFrame with the  pypsa tech group names.</p> required Source code in <code>src/rpycpl/capacities_etl.py</code> <pre><code>def scale_down_pypsa_caps(\n    merged_caps: pd.DataFrame, pypsa_caps: pd.DataFrame, tech_groupings: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale down the pypsa capacities to match the remind capacities by tech group.\n    Does not scale up the pypsa capacities.\n\n    Scaling is done by groups of techs, which allows n:1 mapping of remind to pypsa techs.\n\n    Args:\n        merged_caps (pd.DataFrame): DataFrame with the merged remind and pypsa capacities by tech group.\n        pypsa_caps (pd.DataFrame): DataFrame with the pypsa capacities.\n        tech_groupings (pd.DataFrame): DataFrame with the  pypsa tech group names.\n    \"\"\"\n    merged_caps[\"fraction\"] = merged_caps.capacity_remind / merged_caps.capacity_pypsa\n\n    scalings = merged_caps.copy()\n    # do not touch cases where remind capacity is larger than pypsa capacity\n    scalings[\"fraction\"] = scalings[\"fraction\"].clip(upper=1)\n    scalings.dropna(subset=[\"fraction\"], inplace=True)\n\n    pypsa_caps[\"tech_group\"] = pypsa_caps.Tech.map(tech_groupings.group.to_dict())\n    pypsa_caps = pypsa_caps.merge(\n        scalings[[\"tech_group\", \"fraction\"]],\n        how=\"left\",\n        on=\"tech_group\",\n        suffixes=(\"\", \"_scaling\"),\n    )\n    pypsa_caps.Capacity = pypsa_caps.Capacity * pypsa_caps.fraction\n    return pypsa_caps\n</code></pre>"},{"location":"docs/reference/coupled_cfg/","title":"Coupled cfg","text":"<p>Tools / Script to generate a PyPSA config file for REMIND-PyPSA coupling.</p> <p>Usage:</p> <pre><code>python coupled_cfg.py\n\ncd pypsa_folder\n\nsnakemake --configfile=remind.yaml\n</code></pre> <p>Missing:</p> <ul> <li>arguments or config</li> </ul>"},{"location":"docs/reference/disagg/","title":"Disagg","text":"<p>Disaggregation tools for:</p> <ul> <li>Spatial disaggregation</li> <li>Temporal disaggregation</li> </ul>"},{"location":"docs/reference/disagg/#disagg.SpatialDisaggregator","title":"<code>SpatialDisaggregator</code>","text":"Source code in <code>src/rpycpl/disagg.py</code> <pre><code>class SpatialDisaggregator:\n\n    def __init__(self, targets=None):\n        self._target_nodes = targets\n\n    def validate_reference_data(self, reference_data: pd.Series):\n        \"\"\"Check reference data has expected format\n\n        Args:\n            reference_data (pd.Series): The reference data for disaggregation.\n        Raises:\n            TypeError: If reference data is not a pandas Series.\n            ValueError: If reference data index does not match target nodes.\n            ValueError: If reference data is not normalised to 1.\n        \"\"\"\n\n        if not isinstance(reference_data, pd.Series):\n            raise TypeError(\"Reference data must be a pandas Series\")\n        if self._target_nodes:\n            if not reference_data.index.isin(self._target_nodes).all():\n                raise ValueError(\n                    f\"Reference data index {reference_data.index} does not match target nodes {self._target_nodes}\"\n                )\n        if not reference_data.sum() == 1:\n            raise ValueError(\"Reference data is not normalised to 1\")\n\n    def use_static_reference(self, data: pd.Series, reference_data: pd.Series):\n        \"\"\"\n        Use a reference year to disaggregate the quantity spatially\n\n        Args:\n            data (pd.Series): The data to be disaggregated. Dims: (year,).\n            reference_data (pd.Series): The reference data for disaggregation.\n                E.g the distribution for a reference year. Dims: (space,).\n        Returns:\n            pd.DataFrame: The disaggregated data. Dims: (space, year).\n        \"\"\"\n\n        self.validate_reference_data(reference_data)\n        # outer/cartersian product to get (years, region) matrix\n        return pd.DataFrame(\n            np.outer(data, reference_data),\n            index=data.index,\n            columns=reference_data.index,\n        ).T\n</code></pre>"},{"location":"docs/reference/disagg/#disagg.SpatialDisaggregator.use_static_reference","title":"<code>use_static_reference(data, reference_data)</code>","text":"<p>Use a reference year to disaggregate the quantity spatially</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The data to be disaggregated. Dims: (year,).</p> required <code>reference_data</code> <code>Series</code> <p>The reference data for disaggregation. E.g the distribution for a reference year. Dims: (space,).</p> required <p>Returns:     pd.DataFrame: The disaggregated data. Dims: (space, year).</p> Source code in <code>src/rpycpl/disagg.py</code> <pre><code>def use_static_reference(self, data: pd.Series, reference_data: pd.Series):\n    \"\"\"\n    Use a reference year to disaggregate the quantity spatially\n\n    Args:\n        data (pd.Series): The data to be disaggregated. Dims: (year,).\n        reference_data (pd.Series): The reference data for disaggregation.\n            E.g the distribution for a reference year. Dims: (space,).\n    Returns:\n        pd.DataFrame: The disaggregated data. Dims: (space, year).\n    \"\"\"\n\n    self.validate_reference_data(reference_data)\n    # outer/cartersian product to get (years, region) matrix\n    return pd.DataFrame(\n        np.outer(data, reference_data),\n        index=data.index,\n        columns=reference_data.index,\n    ).T\n</code></pre>"},{"location":"docs/reference/disagg/#disagg.SpatialDisaggregator.validate_reference_data","title":"<code>validate_reference_data(reference_data)</code>","text":"<p>Check reference data has expected format</p> <p>Parameters:</p> Name Type Description Default <code>reference_data</code> <code>Series</code> <p>The reference data for disaggregation.</p> required <p>Raises:     TypeError: If reference data is not a pandas Series.     ValueError: If reference data index does not match target nodes.     ValueError: If reference data is not normalised to 1.</p> Source code in <code>src/rpycpl/disagg.py</code> <pre><code>def validate_reference_data(self, reference_data: pd.Series):\n    \"\"\"Check reference data has expected format\n\n    Args:\n        reference_data (pd.Series): The reference data for disaggregation.\n    Raises:\n        TypeError: If reference data is not a pandas Series.\n        ValueError: If reference data index does not match target nodes.\n        ValueError: If reference data is not normalised to 1.\n    \"\"\"\n\n    if not isinstance(reference_data, pd.Series):\n        raise TypeError(\"Reference data must be a pandas Series\")\n    if self._target_nodes:\n        if not reference_data.index.isin(self._target_nodes).all():\n            raise ValueError(\n                f\"Reference data index {reference_data.index} does not match target nodes {self._target_nodes}\"\n            )\n    if not reference_data.sum() == 1:\n        raise ValueError(\"Reference data is not normalised to 1\")\n</code></pre>"},{"location":"docs/reference/etl/","title":"Etl","text":"<p>ETL TOOL BOX</p> <ul> <li>Abstracted transformations (Transformation, register_etl)</li> <li>ETL registry (list of named conversions)</li> <li>pre-defined conversions (convert_loads, technoeconomic_data)</li> </ul>"},{"location":"docs/reference/etl/#etl.Transformation","title":"<code>Transformation</code>  <code>dataclass</code>","text":"<p>Data class representing the YAML config for the ETL target</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>@dataclass\nclass Transformation:\n    \"\"\"Data class representing the YAML config for the ETL target\"\"\"\n\n    name: str\n    method: Optional[str] = None\n    frames: Dict[str, Any] = field(default_factory=dict)\n    params: Dict[str, Any] = field(default_factory=dict)\n    filters: Dict[str, Any] = field(default_factory=dict)\n    kwargs: Dict[str, Any] = field(default_factory=dict)\n    dependencies: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"docs/reference/etl/#etl.build_tech_groups","title":"<code>build_tech_groups(frames, map_param='investment')</code>","text":"<p>Wrapper for the utils.build_tech_map function</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>@register_etl(\"build_tech_map\")\ndef build_tech_groups(frames, map_param=\"investment\") -&gt; pd.DataFrame:\n    \"\"\" Wrapper for the utils.build_tech_map function\"\"\"\n    return build_tech_map(frames[\"tech_mapping\"], map_param)\n</code></pre>"},{"location":"docs/reference/etl/#etl.convert_loads","title":"<code>convert_loads(loads, region=None)</code>","text":"<p>conversion for loads</p> <p>Parameters:</p> Name Type Description Default <code>loads</code> <code>dict</code> <p>dictionary of dataframes with loads</p> required <code>region</code> <code>(str, Optional)</code> <p>region to filter the data by</p> <code>None</code> <p>Returns:     pd.DataFrame: converted loads (year: load type, value in Mwh)</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>@register_etl(\"convert_load\")\ndef convert_loads(loads: dict[str, pd.DataFrame], region: str = None) -&gt; pd.DataFrame:\n    \"\"\"conversion for loads\n\n    Args:\n        loads (dict): dictionary of dataframes with loads\n        region (str, Optional): region to filter the data by\n    Returns:\n        pd.DataFrame: converted loads (year: load type, value in Mwh)\n    \"\"\"\n    TWYR2MWH = 365 * 24 * 1e6\n    outp = pd.DataFrame()\n    for k, df in loads.items():\n        df[\"load\"] = k.split(\"_\")[0]\n        if (\"region\" in df.columns) &amp; (region is not None):\n            df = df.query(\"region == @region\").drop(columns=[\"region\"])\n        df.value *= TWYR2MWH\n        outp = pd.concat([outp, df], axis=0)\n    return outp.set_index(\"year\")\n</code></pre>"},{"location":"docs/reference/etl/#etl.convert_remind_capacities","title":"<code>convert_remind_capacities(frames, cutoff=0, region=None)</code>","text":"<p>conversion for capacities</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>dict</code> <p>dictionary of dataframes with capacities</p> required <code>region</code> <code>(str, Optional)</code> <p>region to filter the data by</p> <code>None</code> <code>cutoff</code> <code>(int, Optional)</code> <p>min capacity in MW</p> <code>0</code> <p>Returns:     pd.DataFrame: converted capacities (year: load type, value in Mwh)</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>@register_etl(\"convert_capacities\")\ndef convert_remind_capacities(frames: dict[str, pd.DataFrame], cutoff=0, region: str = None) -&gt; pd.DataFrame:\n    \"\"\"conversion for capacities\n\n    Args:\n        frames (dict): dictionary of dataframes with capacities\n        region (str, Optional): region to filter the data by\n        cutoff (int, Optional): min capacity in MW\n    Returns:\n        pd.DataFrame: converted capacities (year: load type, value in Mwh)\n    \"\"\"\n    TW2MW = 1e6\n    caps = frames[\"capacities\"]\n    caps.loc[:, \"value\"] *= TW2MW\n\n    if (\"region\" in caps.columns) &amp; (region is not None):\n        caps = caps.query(\"region == @region\").drop(columns=[\"region\"])\n\n    too_small = caps.query(\"value &lt; @cutoff\").index\n    caps.loc[too_small, \"value\"] = 0\n\n    if \"tech_groups\" in frames:\n        tech_map = frames[\"tech_groups\"]\n        caps.loc[:, \"tech_group\"] = caps.technology.map(tech_map.group.to_dict())\n\n    return caps.rename(columns={\"value\": \"capacity\"}).set_index(\"year\")\n</code></pre>"},{"location":"docs/reference/etl/#etl.register_etl","title":"<code>register_etl(name)</code>","text":"<p>decorator factory to register ETL functions</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>def register_etl(name):\n    \"\"\"decorator factory to register ETL functions\"\"\"\n\n    def decorator(func):\n        ETL_REGISTRY[name] = func\n        return func\n\n    return decorator\n</code></pre>"},{"location":"docs/reference/etl/#etl.technoeconomic_data","title":"<code>technoeconomic_data(frames, mappings, pypsa_costs)</code>","text":"<p>Mapping adapted from Johannes Hemp, based on csv mapping table</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>Dict[str, DataFrame]</code> <p>dictionary of remind frames</p> required <code>mappings</code> <code>DataFrame</code> <p>the mapping dataframe</p> required <code>pypsa_costs</code> <code>DataFrame</code> <p>pypsa costs dataframe</p> required <p>Returns:     pd.DataFrame: dataframe with the mapped techno-economic data</p> Source code in <code>src/rpycpl/etl.py</code> <pre><code>@register_etl(\"technoeconomic_data\")\ndef technoeconomic_data(\n    frames: Dict[str, pd.DataFrame], mappings: pd.DataFrame, pypsa_costs: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Mapping adapted from Johannes Hemp, based on csv mapping table\n\n    Args:\n        frames (Dict[str, pd.DataFrame]): dictionary of remind frames\n        mappings (pd.DataFrame): the mapping dataframe\n        pypsa_costs (pd.DataFrame): pypsa costs dataframe\n    Returns:\n        pd.DataFrame: dataframe with the mapped techno-economic data\n    \"\"\"\n\n    # explode multiple references into rows\n    mappings.loc[:, \"reference\"] = mappings[\"reference\"].apply(to_list)\n\n    # check the data &amp; mappings\n    validate_mappings(mappings)\n\n    # maybe do something nicer but should be ok if remind export is correct\n    years = frames[\"capex\"].year.unique()\n\n    weight_frames = [\n        frames[k].assign(weight_type=k) for k in frames if k.startswith(\"weights\")\n    ]\n    weights = pd.concat(\n        [\n            df.rename(columns={\"carrier\": \"technology\", \"value\": \"weight\"})\n            for df in weight_frames\n        ]\n    )\n\n    costs_remind = make_pypsa_like_costs(frames)\n    costs_remind = costs_remind.merge(weights, on=[\"technology\", \"year\"], how=\"left\")\n\n    validate_remind_data(costs_remind, mappings)\n\n    mappings.loc[:, \"reference\"] = mappings[\"reference\"].apply(to_list)\n\n    # apply the mappings to pypsa tech\n    mapped_costs = map_to_pypsa_tech(\n        remind_costs_formatted=costs_remind,\n        pypsa_costs=pypsa_costs,\n        mappings=mappings,\n        weights=weights,\n        years=years,\n    )\n    mapped_costs[\"value\"].fillna(0, inplace=True)\n    mapped_costs.fillna(\" \", inplace=True)\n\n    return mapped_costs\n</code></pre>"},{"location":"docs/reference/technoecon_etl/","title":"Technoecon etl","text":"<p>Extract data from Remind, transform it for pypsa PyPSA and write it to files</p>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.make_pypsa_like_costs","title":"<code>make_pypsa_like_costs(frames)</code>","text":"<p>translate the REMIND costs into pypsa format for a single region.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>dict[DataFrame]</code> <p>dictionary with the REMIND data tables to be transformed. Region-filtered</p> required <p>Returns:     pd.DataFrame: DataFrame containing cost data for a region.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def make_pypsa_like_costs(\n    frames: dict[pd.DataFrame],\n) -&gt; pd.DataFrame:\n    \"\"\"translate the REMIND costs into pypsa format for a single region.\n\n    Args:\n        frames: dictionary with the REMIND data tables to be transformed. Region-filtered\n    Returns:\n        pd.DataFrame: DataFrame containing cost data for a region.\n    \"\"\"\n\n    # check single region or region already removed\n    regions_filtered = not any([\"region\" in df.columns for df in frames.values()])\n    if not regions_filtered and any(\n        [df.region.nunique() &gt; 1 for df in frames.values() if \"region\" in df.columns]\n    ):\n        raise Warning(\"The dataframes are not region-filtered. Not supported.\")\n    elif not regions_filtered:\n        frames.update(\n            {\n                k: df.drop(columns=[\"region\"])\n                for k, df in frames.items()\n                if \"region\" in df.columns\n            }\n        )\n\n    years = frames[\"capex\"].year.unique()\n    capex = transform_capex(frames[\"capex\"])\n\n    # transform the data\n    vom = transform_vom(frames[\"tech_data\"].query(\"parameter == 'omv'\"))\n    fom = transform_fom(frames[\"tech_data\"].query(\"parameter == 'omf'\"))\n    lifetime = transform_lifetime(frames[\"tech_data\"].query(\"parameter == 'lifetime'\"))\n\n    co2_intens = transform_co2_intensity(frames[\"co2_intensity\"], years)\n    eta = transform_efficiency(frames[\"eta\"], years)\n    fuel_costs = transform_fuels(frames[\"fuel_costs\"])\n    discount_rate = transform_discount_rate(frames[\"discount_r\"])\n\n    del frames\n\n    # stitch together in pypsa format\n    cost_frames = {\n        \"capex\": capex,\n        \"efficiency\": eta,\n        \"fuel\": fuel_costs,\n        \"co2\": co2_intens,\n        \"lifetime\": lifetime,\n        \"vom\": vom,\n        \"fom\": fom,\n        \"discount_rate\": discount_rate,\n    }\n\n    # TODO Can do more efficient operations with join\n\n    # add years to table with time-indep data\n    for label, frame in cost_frames.items():\n        if \"year\" not in frame.columns:\n            cost_frames[label] = expand_years(frame, capex.year.unique())\n    # add missing techs for tech agnostic data\n    for label, frame in cost_frames.items():\n        if \"technology\" not in frame.columns:\n            cost_frames[label] = pd.concat(\n                [frame.assign(technology=tech) for tech in capex.technology.unique()]\n            )\n    column_order = [\"technology\", \"year\", \"parameter\", \"value\", \"unit\", \"source\"]\n\n    # merge the dataframes for the region\n    costs_remind = pd.concat(\n        [frame[column_order] for frame in cost_frames.values()], axis=0\n    ).reset_index(drop=True)\n    costs_remind.sort_values(\n        by=[\"technology\", \"year\", \"parameter\"], key=key_sort, inplace=True\n    )\n\n    return costs_remind\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.map_to_pypsa_tech","title":"<code>map_to_pypsa_tech(remind_costs_formatted, pypsa_costs, mappings, weights, years=None)</code>","text":"<p>Map the REMIND technology names to pypsa technoloies using the conversions specified in the map config</p> <p>Parameters:</p> Name Type Description Default <code>remind_costs_formatted</code> <code>DataFrame</code> <p>DataFrame containing REMIND cost data.</p> required <code>pypsa_costs</code> <code>DataFrame</code> <p>DataFrame containing pypsa cost data.</p> required <code>mappings</code> <code>DataFrame</code> <p>DataFrame containing the mapping funcs and names from REMIND to pypsa technologies.</p> required <code>weights</code> <code>DataFrame</code> <p>DataFrame containing the weights.</p> required <code>years</code> <code>Iterable</code> <p>years to be used. Defaults to None (use remidn dat)</p> <code>None</code> <p>Returns:     pd.DataFrame: DataFrame with mapped technology names.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def map_to_pypsa_tech(\n    remind_costs_formatted: pd.DataFrame,\n    pypsa_costs: pd.DataFrame,\n    mappings: pd.DataFrame,\n    weights: pd.DataFrame,\n    years: list | Iterable = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Map the REMIND technology names to pypsa technoloies using the conversions specified in the\n    map config\n\n    Args:\n        remind_costs_formatted (pd.DataFrame): DataFrame containing REMIND cost data.\n        pypsa_costs (pd.DataFrame): DataFrame containing pypsa cost data.\n        mappings (pd.DataFrame): DataFrame containing the mapping funcs and names from\n            REMIND to pypsa technologies.\n        weights (pd.DataFrame): DataFrame containing the weights.\n        years (Iterable, optional): years to be used. Defaults to None (use remidn dat)\n    Returns:\n        pd.DataFrame: DataFrame with mapped technology names.\n    \"\"\"\n    if years is None:\n        years = remind_costs_formatted.year.unique()\n\n    # direct mapping of remind\n    use_remind = (\n        mappings.query(\"mapper == 'use_remind'\")\n        .drop(\"unit\", axis=1)\n        .merge(\n            remind_costs_formatted,\n            left_on=[\"reference\", \"parameter\"],\n            right_on=[\"technology\", \"parameter\"],\n            how=\"left\",\n        )\n    )\n    use_remind.drop(columns=[\"technology\"], inplace=True)\n\n    direct_input = mappings.query(\"mapper == 'set_value'\").rename(\n        columns={\"reference\": \"value\"}\n    )\n    direct_input = direct_input.assign(source=\"direct_input from coupling mapping\")\n    direct_input = expand_years(direct_input, years)\n\n    # pypsa values\n    from_pypsa = _use_pypsa(mappings, pypsa_costs, years)\n    from_pypsa.drop(columns=[\"technology\"], inplace=True)\n\n    # techs with proxy learnign\n    proxy_learning = _learn_investment_from_proxy(\n        mappings, pypsa_costs, remind_costs_formatted, ref_year=years.min()\n    )\n    if not proxy_learning.empty:\n        proxy_learning.loc[:, \"further description\"] = \"proxy learning from REMIND\"\n    # TODO check weighing is by right quantities\n    # weighed by remind tech basket\n    weighed_basket = _weigh_remind_by(remind_costs_formatted, weights, mappings)\n    # format for output\n    direct_input.rename(\n        columns={\"PyPSA_tech\": \"technology\", \"comment\": \"further description\"},\n        inplace=True,\n    )\n    use_remind.rename(\n        columns={\"PyPSA_tech\": \"technology\", \"comment\": \"further description\"},\n        inplace=True,\n    )\n    from_pypsa.rename(\n        columns={\"PyPSA_tech\": \"technology\", \"comment\": \"further description\"},\n        inplace=True,\n    )\n    proxy_learning.rename(\n        columns={\"PyPSA_tech\": \"technology\", \"comment\": \"further description\"},\n        inplace=True,\n    )\n    weighed_basket.rename(\n        columns={\"PyPSA_tech\": \"technology\", \"comment\": \"further description\"},\n        inplace=True,\n    )\n\n    output_frames = [\n        direct_input,\n        use_remind,\n        from_pypsa,\n        proxy_learning,\n        weighed_basket,\n    ]\n    output = pd.concat([df[OUTP_COLS] for df in output_frames if not df.empty], axis=0)\n    output = output.assign(year=output.year.astype(int))\n    return output.sort_values(\n        [\"year\", \"technology\", \"parameter\"], key=key_sort\n    ).reset_index(drop=True)\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_capex","title":"<code>transform_capex(capex)</code>","text":"<p>Transform the CAPEX data from REMIND to pypsa.</p> <p>Parameters:</p> Name Type Description Default <code>capex</code> <code>DataFrame</code> <p>DataFrame containing REMIND capex data.</p> required <p>Returns:     pd.DataFrame: Transformed capex data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_capex(capex: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform the CAPEX data from REMIND to pypsa.\n\n    Args:\n        capex (pd.DataFrame): DataFrame containing REMIND capex data.\n    Returns:\n        pd.DataFrame: Transformed capex data.\n    \"\"\"\n    capex.loc[:, \"value\"] *= UNIT_CONVERSION[\"capex\"]\n    capex = capex.assign(\n        source=\"REMIND \" + capex.technology, parameter=\"investment\", unit=\"USD/MW\"\n    )\n    store_techs = STOR_TECHS\n    for stor in store_techs:\n        capex.loc[capex[\"technology\"] == stor, \"unit\"] = \"USD/MWh\"\n    return capex\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_co2_intensity","title":"<code>transform_co2_intensity(co2_intensity, years)</code>","text":"<p>Transform the CO2 intensity data from REMIND to pypsa.</p> <p>Parameters:</p> Name Type Description Default <code>co2_intensity</code> <code>DataFrame</code> <p>DataFrame containing REMIND CO2 intensity data.</p> required <code>years</code> <code>list | Index</code> <p>relevant years data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Transformed CO2 intensity data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_co2_intensity(\n    co2_intensity: pd.DataFrame, years: list | pd.Index\n) -&gt; pd.DataFrame:\n    \"\"\"Transform the CO2 intensity data from REMIND to pypsa.\n\n    Args:\n        co2_intensity (pd.DataFrame): DataFrame containing REMIND CO2 intensity data.\n        years (list | pd.Index): relevant years data.\n\n    Returns:\n        pd.DataFrame: Transformed CO2 intensity data.\n    \"\"\"\n    # TODO Co2 equivalent\n    co2_intens = co2_intensity.rename(\n        columns={\n            \"carrier\": \"from_carrier\",\n            \"all_enty_1\": \"to_carrier\",\n            \"all_enty_2\": \"emission_type\",\n            \"all_enty.1\": \"to_carrier\",\n            \"all_enty.2\": \"emission_type\",\n        },\n    )\n    co2_intens = co2_intens.query(\n        \"to_carrier == 'seel' &amp; emission_type == 'co2' &amp; year in @years\"\n    )\n    co2_intens = co2_intens.assign(\n        parameter=\"CO2 intensity\",\n        unit=\"t_CO2/MWh_th\",\n        source=co2_intens.technology + \" REMIND\",\n    )\n    co2_intens.loc[:, \"value\"] *= UNIT_CONVERSION[\"co2_intensity\"]\n    return co2_intens\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_efficiency","title":"<code>transform_efficiency(eff_data, years)</code>","text":"<p>Transform the efficiency data from REMIND to pypsa.</p> <p>Parameters:</p> Name Type Description Default <code>eff_data</code> <code>DataFrame</code> <p>DataFrame containing REMIND efficiency data.</p> required <code>years</code> <code>list | Index</code> <p>relevant years.</p> required <p>Returns:     pd.DataFrame: Transformed efficiency data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_efficiency(\n    eff_data: pd.DataFrame, years: list | pd.Index\n) -&gt; pd.DataFrame:\n    \"\"\"Transform the efficiency data from REMIND to pypsa.\n\n    Args:\n        eff_data (pd.DataFrame): DataFrame containing REMIND efficiency data.\n        years (list | pd.Index): relevant years.\n    Returns:\n        pd.DataFrame: Transformed efficiency data.\n    \"\"\"\n    eta = eff_data.query(\"year in @years\")\n    eta = eta.assign(\n        source=eta.technology + \" REMIND\", unit=\"p.u.\", parameter=\"efficiency\"\n    )\n\n    # Special treatment for nuclear: Efficiencies are in TWa/Mt=8760 TWh/Tg_U\n    #  -&gt; convert to MWh/g_U to match with fuel costs in USD/g_U\n    eta.loc[eta[\"technology\"].isin([\"fnrs\", \"tnrs\"]), \"value\"] *= 8760 / 1e6\n    eta.loc[eta[\"technology\"].isin([\"fnrs\", \"tnrs\"]), \"unit\"] = \"MWh/g_U\"\n    # Special treatment for battery: Efficiencies in costs.csv should be roundtrip\n    eta.loc[eta[\"technology\"] == \"btin\", \"value\"] **= 2\n\n    return eta\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_fom","title":"<code>transform_fom(fom)</code>","text":"<p>Transform the Fixed Operational Maintenance data from REMIND to pypsa.</p> <p>Parameters:</p> Name Type Description Default <code>fom</code> <code>DataFrame</code> <p>DataFrame containing REMIND FOM data.</p> required <p>Returns:     pd.DataFrame: Transformed FOM data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_fom(fom: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform the Fixed Operational Maintenance data from REMIND to pypsa.\n\n    Args:\n        fom (pd.DataFrame): DataFrame containing REMIND FOM data.\n    Returns:\n        pd.DataFrame: Transformed FOM data.\n    \"\"\"\n    fom.loc[:, \"value\"] *= UNIT_CONVERSION[\"FOM\"]\n    fom = fom.assign(source=fom.technology + \" REMIND\")\n    fom = fom.assign(unit=\"percent\", parameter=\"FOM\")\n\n    return fom\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_lifetime","title":"<code>transform_lifetime(lifetime)</code>","text":"<p>Transform the lifetime data from REMIND to pypsa.</p> <p>Parameters:</p> Name Type Description Default <code>lifetime</code> <code>DataFrame</code> <p>DataFrame containing REMIND lifetime data.</p> required <p>Returns:     pd.DataFrame: Transformed lifetime data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_lifetime(lifetime: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform the lifetime data from REMIND to pypsa.\n\n    Args:\n        lifetime (pd.DataFrame): DataFrame containing REMIND lifetime data.\n    Returns:\n        pd.DataFrame: Transformed lifetime data.\n    \"\"\"\n    lifetime = lifetime.assign(\n        unit=\"years\", source=lifetime.technology + \" REMIND\", inplace=True\n    )\n    return lifetime\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.transform_vom","title":"<code>transform_vom(vom)</code>","text":"<p>Transform the Variable Operational Maintenance data from REMIND to pypsa. Args:     vom (pd.DataFrame): DataFrame containing REMIND VOM data. Returns:     pd.DataFrame: Transformed VOM data.</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def transform_vom(vom: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform the Variable Operational Maintenance data from REMIND to pypsa.\n    Args:\n        vom (pd.DataFrame): DataFrame containing REMIND VOM data.\n    Returns:\n        pd.DataFrame: Transformed VOM data.\n    \"\"\"\n    vom.loc[:, \"value\"] *= UNIT_CONVERSION[\"VOM\"]\n    vom = vom.assign(unit=\"USD/MWh\", source=vom.technology + \" REMIND\", parameter=\"VOM\")\n    return vom\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.validate_mappings","title":"<code>validate_mappings(mappings)</code>","text":"<p>validate the mapping of the technologies to pypsa technologies Args:     mappings (pd.DataFrame): DataFrame containing the mapping funcs and names         from REMIND to pypsa technologies. Raises:     ValueError: if mappers not allowed     ValueError: if columns not expected     ValueError: if proxy learning (use_remind_with_learning_from) is used         for something other than invest</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def validate_mappings(mappings: pd.DataFrame):\n    \"\"\"validate the mapping of the technologies to pypsa technologies\n    Args:\n        mappings (pd.DataFrame): DataFrame containing the mapping funcs and names\n            from REMIND to pypsa technologies.\n    Raises:\n        ValueError: if mappers not allowed\n        ValueError: if columns not expected\n        ValueError: if proxy learning (use_remind_with_learning_from) is used\n            for something other than invest\n\n    \"\"\"\n\n    # validate columns\n    EXPECTED_COLUMNS = [\n        \"PyPSA_tech\",\n        \"parameter\",\n        \"mapper\",\n        \"reference\",\n        \"unit\",\n        \"comment\",\n    ]\n    if not sorted(mappings.columns) == sorted(EXPECTED_COLUMNS):\n        raise ValueError(f\"Invalid mapping. Allowed columns are: {EXPECTED_COLUMNS}\")\n\n    # validate mappers allowed\n    forbidden_mappers = set(mappings.mapper.unique()).difference(MAPPING_FUNCTIONS)\n    if forbidden_mappers:\n        raise ValueError(f\"Forbidden mappers found in mappings: {forbidden_mappers}\")\n\n    # validate proxy learning\n    proxy_learning = mappings.query(\"mapper == 'use_remind_with_learning_from'\")\n    proxy_params = set(proxy_learning.parameter)\n    if proxy_params.difference({\"investment\"}):\n        raise ValueError(\n            f\"Proxy learning is only allowed for investment but Found: {proxy_params}\"\n        )\n\n    # validate numeric\n    set_vals = mappings.query(\"mapper == 'set_value'\")[\"reference\"]\n    try:\n        set_vals.astype(float)\n    except ValueError as e:\n        raise ValueError(f\"set_value reference values must be numeric but: {e}\")\n\n    # check uniqueness\n    counts = mappings.groupby([\"PyPSA_tech\", \"parameter\"]).count()\n    repeats = counts[counts.values &gt; 1]\n    if len(repeats):\n        raise ValueError(f\"Mappings are not unique: n repeats:\\n {repeats} \")\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.validate_output","title":"<code>validate_output(df_out, costs_remind)</code>","text":"<p>validate the output data Args:     df_out (pd.DataFrame): DataFrame containing the output data     costs_remind (pd.DataFrame): DataFrame containing the formatted remind data</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def validate_output(df_out: pd.DataFrame, costs_remind: pd.DataFrame):\n    \"\"\"validate the output data\n    Args:\n        df_out (pd.DataFrame): DataFrame containing the output data\n        costs_remind (pd.DataFrame): DataFrame containing the formatted remind data\n    \"\"\"\n\n    missing_vals = df_out.value.isna().any()\n    if missing_vals:\n        raise ValueError(\n            f\"Missing values or nans in output data: {df_out[df_out.value.isna()]}\"\n        )\n\n    n_expected = costs_remind.technology.nunique() * costs_remind.year.nunique()\n</code></pre>"},{"location":"docs/reference/technoecon_etl/#technoecon_etl.validate_remind_data","title":"<code>validate_remind_data(costs_remind, mappings)</code>","text":"<p>validate the remind cost data Args:     remind_data (pd.DataFrame): DataFrame containing the remind data</p> Source code in <code>src/rpycpl/technoecon_etl.py</code> <pre><code>def validate_remind_data(costs_remind: pd.DataFrame, mappings: pd.DataFrame):\n    \"\"\"validate the remind cost data\n    Args:\n        remind_data (pd.DataFrame): DataFrame containing the remind data\n    \"\"\"\n    requested_data = mappings.query(\"mapper.str.contains('remind')\")[\n        [\"PyPSA_tech\", \"parameter\", \"reference\"]\n    ].explode(\"reference\")\n    data = requested_data.explode(\"reference\").merge(\n        costs_remind.rename(columns={\"technology\": \"reference\"}),\n        on=[\"parameter\", \"reference\"],\n        how=\"left\",\n    )\n    data = data[[\"PyPSA_tech\", \"reference\", \"year\", \"parameter\", \"value\"]]\n    missing = data[(data.isna()).any(axis=1)]\n    if not missing.empty:\n        raise ValueError(\n            f\"Missing data in REMIND for (first &lt;10 rows)\\n{missing.drop_duplicates().head(10)}\"\n            \"\\nCheck the mappings and the remind data.\"\n            \" Hint: are your reference lists consistently separated by ',' or ', '?\"\n        )\n</code></pre>"},{"location":"docs/reference/utils/","title":"Utils","text":"<p>Utility functions for the REMIND-PyPSA coupling</p>"},{"location":"docs/reference/utils/#utils.build_tech_map","title":"<code>build_tech_map(remind2pypsa_map, map_param='investment')</code>","text":"<p>Build a mapping from REMIND to PyPSA technology names using the mapping DataFrame. Adds groups in case mapping is not 1:1 Args:     remind2pypsa_map (pd.DataFrame): DataFrame with the (!!validated) mapping     map_param (Optional, str):  the parameter to use for tech name mapping. Defaults to 'investment'. Returns:     pd.DataFrame: DataFrame with the mapping (remind_tech: PyPSA_tech, group)</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>def build_tech_map(\n    remind2pypsa_map: pd.DataFrame, map_param=\"investment\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Build a mapping from REMIND to PyPSA technology names using the mapping DataFrame.\n    Adds groups in case mapping is not 1:1\n    Args:\n        remind2pypsa_map (pd.DataFrame): DataFrame with the (!!validated) mapping\n        map_param (Optional, str):  the parameter to use for tech name mapping. Defaults to 'investment'.\n    Returns:\n        pd.DataFrame: DataFrame with the mapping (remind_tech: PyPSA_tech, group)\n    \"\"\"\n\n    if map_param not in remind2pypsa_map.parameter.unique():\n        raise ValueError(\n            f\"Parameter {map_param} not found in the mapping file. \"\n            \"Please check the mapping file and the parameter name.\"\n        )\n    tech_names_map = remind2pypsa_map.query(\n        \"mapper.str.contains('remind') &amp; not mapper.str.contains('learn') &amp; parameter == @map_param\"\n    )[[\"PyPSA_tech\", \"reference\"]]\n    tech_names_map.rename(columns={\"reference\": \"remind_tech\"}, inplace=True)\n    tech_names_map.loc[:, \"remind_tech\"] = tech_names_map.remind_tech.apply(to_list)\n    tech_names_map = tech_names_map.explode(\"remind_tech\")\n    tech_names_map.set_index(\"remind_tech\", inplace=True)\n    tech_names_map[\"group\"] = tech_names_map.groupby(level=0).PyPSA_tech.apply(\n        lambda x: \" &amp; \".join(x)\n    )\n\n    return tech_names_map\n</code></pre>"},{"location":"docs/reference/utils/#utils.expand_years","title":"<code>expand_years(df, years)</code>","text":"<p>expand the dataframe by the years</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>time-indep data</p> required <code>years</code> <code>list</code> <p>the years</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: time-indep data with explicit years</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>def expand_years(df: pd.DataFrame, years: list) -&gt; pd.DataFrame:\n    \"\"\"expand the dataframe by the years\n\n    Args:\n        df (pd.DataFrame): time-indep data\n        years (list): the years\n\n    Returns:\n        pd.DataFrame: time-indep data with explicit years\n    \"\"\"\n\n    return pd.concat([df.assign(year=yr) for yr in years])\n</code></pre>"},{"location":"docs/reference/utils/#utils.read_gdx","title":"<code>read_gdx(file_path, variable_name, rename_columns={}, error_on_empty=True)</code>","text":"<p>Auxiliary function for standardised and cached reading of REMIND-EU data files to pandas.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>Path to the GDX file.</p> required <code>variable_name</code> <code>str</code> <p>Name of the symbol (param, var, scalar) to read from the GDX file.</p> required <code>rename_columns</code> <code>dict</code> <p>Dictionary for renaming columns. Defaults to {}.</p> <code>{}</code> <code>error_on_empty</code> <code>bool</code> <p>Raise an error if the DataFrame is empty. Defaults to True.</p> <code>True</code> <p>Returns:     pd.DataFrame: the symbol table .</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>@register_reader(\"remind_gdx\")\ndef read_gdx(\n    file_path: os.PathLike, variable_name: str, rename_columns={}, error_on_empty=True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Auxiliary function for standardised and cached reading of REMIND-EU data\n    files to pandas.DataFrame.\n\n    Args:\n        file_path (os.PathLike): Path to the GDX file.\n        variable_name (str): Name of the symbol (param, var, scalar) to read from the GDX file.\n        rename_columns (dict, optional): Dictionary for renaming columns. Defaults to {}.\n        error_on_empty (bool, optional): Raise an error if the DataFrame is empty. Defaults to True.\n    Returns:\n        pd.DataFrame: the symbol table .\n    \"\"\"\n\n    @functools.lru_cache\n    def _read_and_cache_remind_file(fp):\n        return gamspy.Container(load_from=fp)\n\n    data = _read_and_cache_remind_file(file_path)[variable_name]\n\n    df = data.records\n\n    if error_on_empty and (df is None or df.empty):\n        raise ValueError(f\"{variable_name} is empty. In: {file_path}\")\n\n    df = df.rename(columns=rename_columns, errors=\"raise\")\n    df.metdata = data.description\n    return df\n</code></pre>"},{"location":"docs/reference/utils/#utils.read_pypsa_costs","title":"<code>read_pypsa_costs(cost_files, **kwargs)</code>","text":"<p>Read &amp; stitch the pypsa costs files</p> <p>Parameters:</p> Name Type Description Default <code>cost_files</code> <code>list</code> <p>list of paths to the pypsa costs files</p> required <code>**kwargs</code> <code>dict</code> <p>additional arguments for pd.read_csv</p> <code>{}</code> <p>Returns:     pd.DataFrame: the techno-economic data for all years.</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>@register_reader(\"pypsa_costs\")\ndef read_pypsa_costs(cost_files, **kwargs: dict) -&gt; pd.DataFrame:\n    \"\"\"Read &amp; stitch the pypsa costs files\n\n    Args:\n        cost_files (list): list of paths to the pypsa costs files\n        **kwargs: additional arguments for pd.read_csv\n    Returns:\n        pd.DataFrame: the techno-economic data for all years.\n    \"\"\"\n    pypsa_costs = pd.read_csv(cost_files.pop(), **kwargs)\n    for f in cost_files:\n        pypsa_costs = pd.concat([pypsa_costs, pd.read_csv(f)])\n    return pypsa_costs\n</code></pre>"},{"location":"docs/reference/utils/#utils.read_remind_csv","title":"<code>read_remind_csv(file_path, **kwargs)</code>","text":"<p>read an exported csv from remind (a single table of the gam db)</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>path to the csv file</p> required <code>**kwargs</code> <code>dict</code> <p>additional arguments for pd.read_csv</p> <code>{}</code> <p>Returns:     pd.DataFrame: the data.</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>@register_reader(\"remind_csv\")\ndef read_remind_csv(file_path: os.PathLike, **kwargs: dict) -&gt; pd.DataFrame:\n    \"\"\"read an exported csv from remind (a single table of the gam db)\n\n    Args:\n        file_path (os.PathLike): path to the csv file\n        **kwargs: additional arguments for pd.read_csv\n    Returns:\n        pd.DataFrame: the data.\n    \"\"\"\n    df = pd.read_csv(file_path, **kwargs)\n    # in case the parameter depended on the same set, all columns are suffixed with _1, _2, etc.\n    df.columns = df.columns.str.replace(r\"_\\d$\", \"\", regex=True)\n    df.rename(columns=REMIND_NAME_MAP, inplace=True)\n\n    df.columns = _fix_repeated_columns(df.columns)\n\n    if \"value\" in df.columns:\n        df.loc[:, \"value\"] = df.value.astype(float)\n\n    return df\n</code></pre>"},{"location":"docs/reference/utils/#utils.read_remind_descriptions_csv","title":"<code>read_remind_descriptions_csv(file_path)</code>","text":"<p>read the exported description from remind</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>csv export from gamsconnect/embedded python</p> required <p>Returns:     pd.DataFrame: the descriptors per symbol, with units extracted</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>@register_reader(\"remind_descriptions\")\ndef read_remind_descriptions_csv(file_path: os.PathLike) -&gt; pd.DataFrame:\n    \"\"\"read the exported description from remind\n\n    Args:\n        file_path (os.PathLike): csv export from gamsconnect/embedded python\n    Returns:\n        pd.DataFrame: the descriptors per symbol, with units extracted\n    \"\"\"\n\n    descriptors = pd.read_csv(file_path)\n    descriptors[\"unit\"] = descriptors[\"text\"].str.extract(r\"\\[(.*?)\\]\")\n    return descriptors.rename(columns={\"Unnamed: 0\": \"symbol\"}).fillna(\"\")\n</code></pre>"},{"location":"docs/reference/utils/#utils.read_remind_regions_csv","title":"<code>read_remind_regions_csv(mapping_path, separator=',')</code>","text":"<p>read the export from remind</p> <p>Parameters:</p> Name Type Description Default <code>mapping_path</code> <code>PathLike</code> <p>the path to the remind mapping (csv export of regi2iso set via GamsConnect)</p> required <code>separator</code> <code>str</code> <p>the separator in the csv. Defaults to \",\".</p> <code>','</code> <p>Returns:     pd.DataFrame: the region mapping</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>@register_reader(\"remind_regions\")\ndef read_remind_regions_csv(mapping_path: os.PathLike, separator=\",\") -&gt; pd.DataFrame:\n    \"\"\"read the export from remind\n\n    Args:\n        mapping_path (os.PathLike): the path to the remind mapping (csv export of regi2iso set via GamsConnect)\n        separator (str, optional): the separator in the csv. Defaults to \",\".\n    Returns:\n        pd.DataFrame: the region mapping\n    \"\"\"\n    regions = pd.read_csv(mapping_path)\n    regions.drop(columns=\"element_text\", inplace=True)\n    regions[\"iso2\"] = coco.convert(regions[\"iso\"], to=\"ISO2\")\n    return regions\n</code></pre>"},{"location":"docs/reference/utils/#utils.register_reader","title":"<code>register_reader(name)</code>","text":"<p>decorator factory to register ETL functions</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>def register_reader(name):\n    \"\"\"decorator factory to register ETL functions\"\"\"\n\n    def decorator(func):\n        READERS_REGISTRY[name] = func\n        return func\n\n    return decorator\n</code></pre>"},{"location":"docs/reference/utils/#utils.to_list","title":"<code>to_list(x)</code>","text":"<p>in case of csv input. conver str to list</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>maybe list like string</p> required Source code in <code>src/rpycpl/utils.py</code> <pre><code>def to_list(x: str):\n    \"\"\"in case of csv input. conver str to list\n\n    Args:\n        x (str): maybe list like string\"\"\"\n    if isinstance(x, str) and x.startswith(\"[\") and x.endswith(\"]\"):\n        split = x.replace(\"[\", \"\").replace(\"]\", \"\").split(\", \")\n        # in case no space in the text-list sep\n        if split[0].find(\",\") &gt;= 0:\n            return x.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n        else:\n            return split\n    return x\n</code></pre>"},{"location":"docs/reference/utils/#utils.validate_file_list","title":"<code>validate_file_list(file_list)</code>","text":"<p>Validate the file list to ensure all files exist.</p> Source code in <code>src/rpycpl/utils.py</code> <pre><code>def validate_file_list(file_list):\n    \"\"\"Validate the file list to ensure all files exist.\"\"\"\n    for file in file_list:\n        if not os.path.isfile(file):\n            raise FileNotFoundError(f\"File {file} does not exist.\")\n</code></pre>"},{"location":"docs/reference/utils/#utils.write_cost_data","title":"<code>write_cost_data(cost_data, output_dir, descript=None)</code>","text":"<p>Write the cost data to a folder, with one CSV file per year.</p> <p>Parameters:</p> Name Type Description Default <code>cost_data</code> <code>DataFrame</code> <p>The cost data to write.</p> required <code>output_dir</code> <code>PathLike</code> <p>The directory to write the file to.</p> required <code>descript</code> <code>str</code> <p>optioal description to add to the file name</p> <code>None</code> Source code in <code>src/rpycpl/utils.py</code> <pre><code>def write_cost_data(\n    cost_data: pd.DataFrame, output_dir: os.PathLike, descript: str = None\n):\n    \"\"\"Write the cost data to a folder, with one CSV file per year.\n\n    Args:\n        cost_data (pd.DataFrame): The cost data to write.\n        output_dir (os.PathLike): The directory to write the file to.\n        descript (str, optional): optioal description to add to the file name\n    \"\"\"\n\n    if descript:\n        output_dir += f\"{descript}\"\n    for year, group in cost_data.groupby(\"year\"):\n        export_p = os.path.join(output_dir, f\"costs_{year}.csv\")\n        group.to_csv(export_p, index=False)\n</code></pre>"}]}